{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/all.txt', encoding='utf8') as f:\n",
    "    s = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = s.split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_len(text):\n",
    "    return  len(text) >= 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_articles = list(filter(filter_len, articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12545"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33808"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(article):\n",
    "    sentences = sent_tokenize(article)\n",
    "    new_sentences = []\n",
    "    cur_sentence = ''\n",
    "    for sentence in sentences:\n",
    "        if len(cur_sentence) + len(sentence) > 700:\n",
    "            new_sentences.append(cur_sentence)\n",
    "            cur_sentence = sentence\n",
    "        else:\n",
    "            cur_sentence += ' {}'.format(sentence)\n",
    "    new_sentences.append(cur_sentence)\n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for article in new_articles:\n",
    "    articles.extend(extract_sentences(article))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sum([len(sent) for sent in articles])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('all.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n\\n'.join(articles))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '\\n\\n'.join(new_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95740255"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(sent) for sent in articles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n\\n'.join(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '\\n\\n'.join(new_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3268149"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Оруо мас ортотуттан айдаан-куйдаан, этиһии күөдьүйүөн сөп. Онон сытыы тылгын кыана тутун,\\nдьон баарыгар эргин да, ойоххун да куһаҕаннык саҥарыма. Үп-харчы чааһыгар сүтүк баар буолуон сөп. Устудьуоннар саамай табыллар нэдиэлэлэрэ. Турист быһыытынан сынньана айанныырга эмиэ ордук тоҕоостоох күннэр.\\nОҕус\\n\\nАраас мэһэй-таһай эйигиттэн тутулуга суох буолуоҕа. Эн тугу да гыммытыҥ иһин, бүтэй хаһаа иһиттэн тахсарыҥ өссө да эрдэ. Хата, харчы баар буолуо. Элбэх. Уустук кэмҥэ эти-сиини эрчийэн, өссө тулуурдаах, кыахтаах буоларга дьулуургун улаатыннарыаххын сөп.\\nИгирэлэр\\n\\nДоруобуйаҕын күүскэ көрүн. Хаан баттааһына уларыйара түргэнэ сүрдээх. Ыараханы көтөҕөлөөмө. Ханна да ыксаан-бохсоон сүүрэкэлээмэ. Соччо билбэт эйгэҕэр билэр баҕайы курдук туттан-хаптан ылсарыҥ табыллыбат. Сүүскэ бэрдэриэҥ.\\nАраак\\n\\nХарчыҥ кырыымчык. Иэс-күүс, кирэдьиит күүскэ ыкпыт. Саҥа кирэдьииккэ аккаастыахтарын сөп. Таптыыр киһигиттэн олус элбэҕи эрэйэр эбиккин. Онон ити икки суолга болҕомтоҕун уурбакка, күннээҕигин бөрөөтөххүн'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov import configs, build_model\n",
    "from deeppavlov.core.commands.utils import parse_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-29 11:42:05.752 INFO in 'deeppavlov.core.data.utils'['utils'] at line 80: Downloading from http://files.deeppavlov.ai/deeppavlov_data/ner_rus_bert_v1.tar.gz to C:\\Users\\petro\\.deeppavlov\\ner_rus_bert_v1.tar.gz\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.32G/1.32G [21:27<00:00, 1.02MB/s]\n",
      "2020-05-29 12:03:32.970 INFO in 'deeppavlov.core.data.utils'['utils'] at line 242: Extracting C:\\Users\\petro\\.deeppavlov\\ner_rus_bert_v1.tar.gz archive into C:\\Users\\petro\\.deeppavlov\\models\n",
      "2020-05-29 12:03:44.369 INFO in 'deeppavlov.core.data.utils'['utils'] at line 80: Downloading from http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_v1.tar.gz to C:\\Users\\petro\\.deeppavlov\\downloads\\rubert_cased_L-12_H-768_A-12_v1.tar.gz\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 666M/666M [04:20<00:00, 2.55MB/s]\n",
      "2020-05-29 12:08:05.249 INFO in 'deeppavlov.core.data.utils'['utils'] at line 242: Extracting C:\\Users\\petro\\.deeppavlov\\downloads\\rubert_cased_L-12_H-768_A-12_v1.tar.gz archive into C:\\Users\\petro\\.deeppavlov\\downloads\\bert_models\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\petro\\pycharmprojects\\ner-sakha\\venv\\lib\\site-packages\\bert_dp\\tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-29 12:08:11.1 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from C:\\Users\\petro\\.deeppavlov\\models\\ner_rus_bert\\tag.dict]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\petro\\pycharmprojects\\ner-sakha\\venv\\lib\\site-packages\\bert_dp\\modeling.py:418: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\petro\\pycharmprojects\\ner-sakha\\venv\\lib\\site-packages\\bert_dp\\modeling.py:499: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\petro\\pycharmprojects\\ner-sakha\\venv\\lib\\site-packages\\bert_dp\\modeling.py:366: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From c:\\users\\petro\\pycharmprojects\\ner-sakha\\venv\\lib\\site-packages\\bert_dp\\modeling.py:283: The name tf.erf is deprecated. Please use tf.math.erf instead.\n",
      "\n",
      "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
      "WARNING:tensorflow:From c:\\users\\petro\\pycharmprojects\\ner-sakha\\venv\\lib\\site-packages\\deeppavlov\\core\\models\\tf_model.py:94: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\petro\\pycharmprojects\\ner-sakha\\venv\\lib\\site-packages\\deeppavlov\\models\\bert\\bert_sequence_tagger.py:671: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-29 12:08:30.277 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from C:\\Users\\petro\\.deeppavlov\\models\\ner_rus_bert\\model]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\petro\\.deeppavlov\\models\\ner_rus_bert\\model\n"
     ]
    }
   ],
   "source": [
    "model = build_model(configs.ner.ner_rus_bert, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/corpus.txt', encoding='utf8') as f:\n",
    "    s = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [line.split(' ||| ')[1] for line in s if len(line.split(' ||| ')) == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'В середине ролика может разгорелся скандал и высказывание . Так что острое слово , не говори плохо ни людям , ни жене . В финансовой части могут быть потери . Самая удачная неделя для студентов . Наиболее благоприятные дни для отдыха в качестве туриста . Бык'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [model([sentence])[1][0] for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentences_tmp = [model([sentence])[0][0] for sentence in sentences] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('data/dataset.txt', encoding='utf8') as f:\n",
    "    s = f.read().split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('data/corpus.txt', encoding='utf8') as f:\n",
    "    corpus = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [line for line in corpus if line != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "orig_sentences = [line.split(' ||| ')[0] for line in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_sentences = [' '.join([token.split(' ')[0] for token in tokens.split('\\n')]) for tokens in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentences = [' ||| '.join([new_sentences[i], orig_sentences[i]]) for i in range(len(new_sentences))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('data/new_corpus.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/dataset.txt', encoding='utf8') as f:\n",
    "    s = f.read().split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/corpus.txt', encoding='utf8') as f:\n",
    "    corpus = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18507"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18506"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [line for line in corpus if line != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18506"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_sentences = [line.split(' ||| ')[0] for line in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentences = [' '.join([token.split(' ')[0] for token in tokens.split('\\n')]) for tokens in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [' ||| '.join([new_sentences[i], orig_sentences[i]]) for i in range(len(new_sentences))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'В середине ролика может разгорелся скандал и высказывание . Так что острое слово , не говори плохо ни людям , ни жене . В финансовой части могут быть потери . Самая удачная неделя для студентов . Наиболее благоприятные дни для отдыха в качестве туриста . Бык ||| Оруо мас ортотуттан айдаан - куйдаан , этиһии күөдьүйүөн сөп . Онон сытыы тылгын кыана тутун , дьон баарыгар эргин да , ойоххун да куһаҕаннык саҥарыма . Үп - харчы чааһыгар сүтүк баар буолуон сөп . Устудьуоннар саамай табыллар нэдиэлэлэрэ . Турист быһыытынан сынньана айанныырга эмиэ ордук тоҕоостоох күннэр . Оҕус'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-13-6cd3031936bb>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'data/new_corpus.txt'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'w'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'utf8'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m     \u001B[0mf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'\\n'\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msentences\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "with open('data/new_corpus.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/collect3/data.txt', encoding='utf8') as f:\n",
    "    data = f.read().split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid, test = train_test_split(test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/collect3/train.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n\\n'.join(train))\n",
    "with open('data/collect3/valid.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n\\n'.join(valid))\n",
    "with open('data/collect3/test.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n\\n'.join(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12028"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}